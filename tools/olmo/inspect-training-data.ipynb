{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58943b11",
   "metadata": {},
   "source": [
    "# Inspect OLMo training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babad8d2",
   "metadata": {},
   "source": [
    "### requires https://github.com/allenai/OLMo\n",
    "#### here we provide a class to download individual batches of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b481dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# exponential backoff\n",
    "from tenacity import retry\n",
    "\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "\n",
    "from olmo import TrainConfig\n",
    "from olmo.data import build_train_dataloader\n",
    "from olmo.tokenizer import Tokenizer\n",
    "\n",
    "import requests\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "@retry(wait='exponential', stop=(10, 60))\n",
    "def download_chunk(url, start_byte, end_byte):\n",
    "    headers = {'Range': f'bytes={start_byte}-{end_byte}'}\n",
    "    response = requests.get(url, headers=headers, stream=True)\n",
    "    if response.status_code == 206:  # 206 indicates a successful partial content request\n",
    "        return response.content\n",
    "    else:\n",
    "        raise ValueError(f\"Failed to download chunk from {url} with status code {response.status_code}\")\n",
    "\n",
    "\n",
    "def download_dataset_chunk(dataset, url:str, index :int):\n",
    "    dtype = dataset.dtype\n",
    "    item_size = dtype(0).itemsize\n",
    "    bytes_start = index * item_size * dataset._chunk_size\n",
    "    num_bytes = item_size * dataset._chunk_size\n",
    "    batch_bytes = download_chunk(url, bytes_start, bytes_start+num_bytes-1)\n",
    "    return np.frombuffer(batch_bytes, dtype=dataset.dtype).tolist()\n",
    "\n",
    "\n",
    "def download_dataset_chunks_simultaneously(dataset, metadata, max_workers=8, max_retries=5):\n",
    "    \"\"\"\n",
    "    Asynchronously download different sequences in the batch with retry logic for failed chunks.\n",
    "    \"\"\"        \n",
    "    # First attempt: try to download all chunks\n",
    "    futures = {}\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit initial tasks\n",
    "        for i, x in enumerate(metadata):\n",
    "            future = executor.submit(download_dataset_chunk, dataset, x[0], x[1])\n",
    "            futures[future] = (i, x)  # Store both index and metadata\n",
    "        \n",
    "        # Create results list\n",
    "        results = [None] * len(futures)\n",
    "        failed_chunks = []\n",
    "        \n",
    "        # Process completed futures\n",
    "        for future in as_completed(futures):\n",
    "            index, chunk_metadata = futures[future]\n",
    "            try:\n",
    "                results[index] = future.result()\n",
    "            except Exception as e:\n",
    "                print(f\"Initial download failed for chunk at index {index}: {e}\")\n",
    "                failed_chunks.append((index, chunk_metadata))\n",
    "    \n",
    "    # Retry failed chunks with exponential backoff\n",
    "    retry_count = 0\n",
    "    while failed_chunks and retry_count < max_retries:\n",
    "        retry_count += 1\n",
    "        print(f\"Retry attempt {retry_count} for {len(failed_chunks)} failed chunks...\")\n",
    "        \n",
    "        still_failed = []\n",
    "        with ThreadPoolExecutor(max_workers=min(max_workers, len(failed_chunks))) as executor:\n",
    "            retry_futures = {}\n",
    "            \n",
    "            for index, chunk_metadata in failed_chunks:\n",
    "                future = executor.submit(download_dataset_chunk, dataset, chunk_metadata[0], chunk_metadata[1])\n",
    "                retry_futures[future] = (index, chunk_metadata)\n",
    "            \n",
    "            for future in as_completed(retry_futures):\n",
    "                index, chunk_metadata = retry_futures[future]\n",
    "                try:\n",
    "                    results[index] = future.result()\n",
    "                    print(f\"Successfully downloaded chunk {index} on retry {retry_count}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Retry {retry_count} failed for chunk {index}: {e}\")\n",
    "                    still_failed.append((index, chunk_metadata))\n",
    "        \n",
    "        failed_chunks = still_failed\n",
    "        \n",
    "        # Add exponential backoff between retry rounds\n",
    "        if failed_chunks and retry_count < max_retries:\n",
    "            wait_time = min(2 ** retry_count, 30)  # Cap at 30 seconds\n",
    "            print(f\"Waiting {wait_time}s before next retry round...\")\n",
    "            time.sleep(wait_time)\n",
    "    \n",
    "    # Report final failures\n",
    "    if failed_chunks:\n",
    "        failed_indices = [idx for idx, _ in failed_chunks]\n",
    "        print(f\"Warning: {len(failed_chunks)} chunks failed after {max_retries} retry attempts: {failed_indices}\")\n",
    "\n",
    "    # Check for any None results and raise exception if found\n",
    "    none_indices = [i for i, result in enumerate(results) if result is None]\n",
    "    if none_indices:\n",
    "        raise RuntimeError(f\"Failed to download chunks at indices: {none_indices}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "class OLMoBatchDownloader:\n",
    "    \"\"\"A simple helper class to download individual batches from the OLMo training data\"\"\"\n",
    "\n",
    "    def __init__(self, olmo_config_path: str, device_train_batch_size=2):\n",
    "        \"\"\"Initialize the OLMoBatchDownloader with the path to the OLMo configuration file.\"\"\"\n",
    "        self.cfg = TrainConfig.load(olmo_config_path)\n",
    "        self.sequence_length = self.cfg.model.max_sequence_length\n",
    "        self.tokenizer = Tokenizer.from_train_config(self.cfg)\n",
    "        self.cfg.device_train_batch_size = device_train_batch_size # if we do not set this we get an assertion error in build_train_dataloader\n",
    "        self.cfg.save_overwrite = True # if we do not set this, we get an error if the folder already exists. might want to change this in the future.\n",
    "        self.dataloader = build_train_dataloader(self.cfg)\n",
    "        self.dataset = self.dataloader.dataset\n",
    "        self.indices = self.dataset.get_global_indices()\n",
    "        self.batch_size = self.cfg.global_train_batch_size\n",
    "\n",
    "\n",
    "    def get_item_metadata(self, index: int):\n",
    "        \"\"\"Get the metadata for all instances in a batch. Extracted from memmap_dataset.py.\"\"\"\n",
    "        index = int(index)  # in case this is a numpy int type.\n",
    "        pos_index = index if index >= 0 else len(self.dataset.dataset) + index\n",
    "\n",
    "        # The index of the memmap array within 'self.memmaps'\n",
    "        memmap_index: Optional[int] = None\n",
    "        # The 'index' relative to the corresponding memmap array.\n",
    "        memmap_local_index: Optional[int] = None\n",
    "        for i, (offset_start, offset_end) in enumerate(self.dataset.dataset.offsets):\n",
    "            if offset_start <= pos_index < offset_end:\n",
    "                memmap_index = i\n",
    "                memmap_local_index = pos_index - offset_start\n",
    "\n",
    "        if memmap_index is None or memmap_local_index is None:\n",
    "            raise IndexError(f\"{index} is out of bounds for dataset of size {len(self.dataset.dataset)}\")\n",
    "\n",
    "        # Read the data from file.\n",
    "        return self.dataset.dataset._memmap_paths[memmap_index], memmap_local_index\n",
    "\n",
    "\n",
    "    def get_batch_metadata(self, batch_idx: int):\n",
    "        batch_start = batch_idx * self.batch_size\n",
    "        batch_end = (batch_idx + 1) * self.batch_size\n",
    "        batch_indices = self.indices[batch_start:batch_end]\n",
    "        batch_metadata = []\n",
    "        for index in batch_indices:\n",
    "            batch_metadata.append(self.get_item_metadata(index))\n",
    "        return batch_metadata\n",
    "\n",
    "\n",
    "    def download_batch(self, batch_idx: int, max_workers=8):\n",
    "        batch_ids = download_dataset_chunks_simultaneously(self.dataset.dataset, self.get_batch_metadata(batch_idx), max_workers)\n",
    "        batch_texts = [self.tokenizer.decode(ids, skip_special_tokens=False) for ids in batch_ids]\n",
    "        return batch_texts, batch_ids\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1508f7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo_config_path = \"../../../../configs/official-0425/OLMo2-1B-stage1.yaml\" # path to the configuration file of the training run\n",
    "\n",
    "batch_downloader = OLMoBatchDownloader(olmo_config_path, device_train_batch_size=512) # device_train_batch_size is a required dummy, this code does not require a gpu\n",
    "\n",
    "batch_idx = 252 # Example batch index to download\n",
    "batch_text, batch_tokens = batch_downloader.download_batch(batch_idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "olmo-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
