{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token Insertion Module Demo\n",
    "\n",
    "This notebook demonstrates how to use the `token_insertion` module to insert token sequences into training data at specific or random positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pretrain_experiments.token_insertion import (\n",
    "    IntervalSet,\n",
    "    wrap_sequences_in_eos_tokens,\n",
    "    add_explicit_insertions,\n",
    "    add_random_insertions,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Typical values for OLMo-2\n",
    "SEQUENCE_LENGTH = 4096\n",
    "EOS_TOKEN_ID = 100257\n",
    "\n",
    "# Example token sequences (in practice, these come from tokenizer.encode())\n",
    "token_sequences = [\n",
    "    [1000, 2000, 3000],\n",
    "    [100, 200],\n",
    "    [42, 43, 44, 45],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The insert_dict Structure\n",
    "\n",
    "The core output of the insertion functions is an `insert_dict` - a dictionary that maps **global token positions** to **token sequences**:\n",
    "\n",
    "```python\n",
    "insert_dict = {\n",
    "    global_position: [token_id, token_id, ...],\n",
    "    global_position: [token_id, token_id, ...],\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "This dict tells the training framework: \"at position X in the training data stream, insert these tokens\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Explicit Insertions\n",
    "\n",
    "With explicit insertions, you specify exactly where each sequence goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "insert_dict =\n",
      "{\n",
      "    1000: [1000, 2000, 3000],\n",
      "    5000: [100, 200],\n",
      "    10000: [42, 43, 44, 45],\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "positions = [1000, 5000, 10000]\n",
    "\n",
    "insert_dict, _ = add_explicit_insertions(\n",
    "    token_sequences=token_sequences,\n",
    "    positions=positions,\n",
    "    existing_insertions=IntervalSet()\n",
    ")\n",
    "\n",
    "print(\"insert_dict =\")\n",
    "print(\"{\")\n",
    "for pos, tokens in sorted(insert_dict.items()):\n",
    "    print(f\"    {pos}: {tokens},\")\n",
    "print(\"}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overlapping insertions raise ValueError\n",
    "\n",
    "If you try to insert at overlapping positions, a `ValueError` is raised:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValueError: Insertion at position 1002 (length 3) overlaps with existing insertion\n"
     ]
    }
   ],
   "source": [
    "interval_set = IntervalSet()\n",
    "\n",
    "# First insertion at position 1000 (length 3, so occupies 1000-1002)\n",
    "insert_dict, interval_set = add_explicit_insertions(\n",
    "    token_sequences=[[1, 2, 3]],\n",
    "    positions=[1000],\n",
    "    existing_insertions=interval_set\n",
    ")\n",
    "\n",
    "# Try overlapping insertion at position 1002 - this will raise ValueError\n",
    "try:\n",
    "    add_explicit_insertions(\n",
    "        token_sequences=[[4, 5, 6]],\n",
    "        positions=[1002],\n",
    "        existing_insertions=interval_set\n",
    "    )\n",
    "except ValueError as e:\n",
    "    print(f\"ValueError: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random Insertions\n",
    "\n",
    "With random insertions, positions are chosen automatically within a range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 30690.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of inserted tokens: 9\n",
      "Avoided collisions while inserting sequences: 0\n",
      "insert_dict =\n",
      "{\n",
      "    88696: [100, 200],\n",
      "    156013: [1000, 2000, 3000],\n",
      "    173804: [42, 43, 44, 45],\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "insert_dict, _ = add_random_insertions(\n",
    "    token_sequences=token_sequences,\n",
    "    start_idx=0,\n",
    "    end_idx=50 * SEQUENCE_LENGTH,\n",
    "    sequence_length=SEQUENCE_LENGTH,\n",
    "    existing_insertions=IntervalSet(),\n",
    "    rng=np.random.default_rng(seed=42)\n",
    ")\n",
    "\n",
    "print(\"insert_dict =\")\n",
    "print(\"{\")\n",
    "for pos, tokens in sorted(insert_dict.items()):\n",
    "    print(f\"    {pos}: {tokens},\")\n",
    "print(\"}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. With EOS Wrapping\n",
    "\n",
    "Typically you wrap sequences with EOS tokens before insertion to cleanly separate them from surrounding content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum sequence length: 2\n",
      "Maximum sequence length: 4\n",
      "Second maximum sequence length: 3\n",
      "Dropped 0 overly long sequences (longer than 4096 tokens).\n",
      "Dropped 0 empty sequences.\n",
      "Minimum sequence length after wrapping: 4\n",
      "Maximum sequence length after wrapping: 6\n",
      "Second maximum sequence length after wrapping: 5\n",
      "Before wrapping:\n",
      "  [1000, 2000, 3000]\n",
      "  [100, 200]\n",
      "  [42, 43, 44, 45]\n",
      "\n",
      "After wrapping (EOS = 100257):\n",
      "  [100257, 1000, 2000, 3000, 100257]\n",
      "  [100257, 100, 200, 100257]\n",
      "  [100257, 42, 43, 44, 45, 100257]\n"
     ]
    }
   ],
   "source": [
    "# First wrap with EOS tokens\n",
    "wrapped = wrap_sequences_in_eos_tokens(token_sequences, SEQUENCE_LENGTH, EOS_TOKEN_ID)\n",
    "\n",
    "print(\"Before wrapping:\")\n",
    "for seq in token_sequences:\n",
    "    print(f\"  {seq}\")\n",
    "\n",
    "print(\"\\nAfter wrapping (EOS = 100257):\")\n",
    "for seq in wrapped:\n",
    "    print(f\"  {seq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 34007.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of inserted tokens: 15\n",
      "Avoided collisions while inserting sequences: 0\n",
      "Final insert_dict (with EOS tokens):\n",
      "{\n",
      "    88695: [100257, 100, 200, 100257],\n",
      "    156013: [100257, 1000, 2000, 3000, 100257],\n",
      "    173803: [100257, 42, 43, 44, 45, 100257],\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Then insert\n",
    "insert_dict, _ = add_random_insertions(\n",
    "    token_sequences=wrapped,\n",
    "    start_idx=0,\n",
    "    end_idx=50 * SEQUENCE_LENGTH,\n",
    "    sequence_length=SEQUENCE_LENGTH,\n",
    "    existing_insertions=IntervalSet(),\n",
    "    rng=np.random.default_rng(seed=42)\n",
    ")\n",
    "\n",
    "print(\"Final insert_dict (with EOS tokens):\")\n",
    "print(\"{\")\n",
    "for pos, tokens in sorted(insert_dict.items()):\n",
    "    print(f\"    {pos}: {tokens},\")\n",
    "print(\"}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Combining Explicit and Random\n",
    "\n",
    "Process explicit insertions first, then random. The random insertions automatically avoid the explicit positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 28149.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of inserted tokens: 4\n",
      "Avoided collisions while inserting sequences: 0\n",
      "Combined insert_dict:\n",
      "{\n",
      "    8192: [1, 2, 3],  # explicit\n",
      "    19064: [30, 40],  # random\n",
      "    29037: [10, 20],  # random\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "interval_set = IntervalSet()\n",
    "\n",
    "# Phase 1: Explicit\n",
    "explicit_dict, interval_set = add_explicit_insertions(\n",
    "    token_sequences=[[1, 2, 3]],\n",
    "    positions=[8192],  # Exactly at position 8192\n",
    "    existing_insertions=interval_set\n",
    ")\n",
    "\n",
    "# Phase 2: Random (uses the same interval_set, so won't overlap)\n",
    "random_dict, interval_set = add_random_insertions(\n",
    "    token_sequences=[[10, 20], [30, 40]],\n",
    "    start_idx=0,\n",
    "    end_idx=10 * SEQUENCE_LENGTH,\n",
    "    sequence_length=SEQUENCE_LENGTH,\n",
    "    existing_insertions=interval_set,\n",
    "    rng=np.random.default_rng(seed=42)\n",
    ")\n",
    "\n",
    "# Combine\n",
    "insert_dict = {**explicit_dict, **random_dict}\n",
    "\n",
    "print(\"Combined insert_dict:\")\n",
    "print(\"{\")\n",
    "for pos, tokens in sorted(insert_dict.items()):\n",
    "    source = \"explicit\" if pos in explicit_dict else \"random\"\n",
    "    print(f\"    {pos}: {tokens},  # {source}\")\n",
    "print(\"}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The `insert_dict` is a simple mapping:\n",
    "\n",
    "```\n",
    "{position: tokens, position: tokens, ...}\n",
    "```\n",
    "\n",
    "This gets passed to the framework (e.g., OLMo) which handles the actual insertion into the training data stream."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "olmo-core",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
