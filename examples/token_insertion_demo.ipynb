{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token Insertion Module Demo\n",
    "\n",
    "This notebook demonstrates how to use the `token_insertion` module to insert token sequences into training data at specific or random positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pretrain_experiments.token_insertion import (\n",
    "    IntervalSet,\n",
    "    wrap_sequences_in_eos_tokens,\n",
    "    add_explicit_insertions,\n",
    "    add_random_insertions,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Typical values for OLMo-2\n",
    "SEQUENCE_LENGTH = 4096\n",
    "EOS_TOKEN_ID = 100257\n",
    "\n",
    "# Example token sequences (in practice, these come from tokenizer.encode())\n",
    "token_sequences = [\n",
    "    [1000, 2000, 3000],\n",
    "    [100, 200],\n",
    "    [42, 43, 44, 45],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The insert_dict Structure\n",
    "\n",
    "The core output of the insertion functions is an `insert_dict` - a dictionary that maps **global token positions** to **token sequences**:\n",
    "\n",
    "```python\n",
    "insert_dict = {\n",
    "    global_position: [token_id, token_id, ...],\n",
    "    global_position: [token_id, token_id, ...],\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "This dict tells the training framework: \"at position X in the training data stream, insert these tokens\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Explicit Insertions\n",
    "\n",
    "With explicit insertions, you specify exactly where each sequence goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "insert_dict =\n",
      "{\n",
      "    1000: [1000, 2000, 3000],\n",
      "    5000: [100, 200],\n",
      "    10000: [42, 43, 44, 45],\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "positions = [1000, 5000, 10000]\n",
    "\n",
    "insert_dict, _ = add_explicit_insertions(\n",
    "    token_sequences=token_sequences,\n",
    "    positions=positions,\n",
    "    existing_insertions=IntervalSet()\n",
    ")\n",
    "\n",
    "print(\"insert_dict =\")\n",
    "print(\"{\")\n",
    "for pos, tokens in sorted(insert_dict.items()):\n",
    "    print(f\"    {pos}: {tokens},\")\n",
    "print(\"}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overlapping insertions raise ValueError\n",
    "\n",
    "If you try to insert at overlapping positions, a `ValueError` is raised:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValueError: Insertion at position 1002 (length 3) overlaps with existing insertion\n"
     ]
    }
   ],
   "source": [
    "interval_set = IntervalSet()\n",
    "\n",
    "# First insertion at position 1000 (length 3, so occupies 1000-1002)\n",
    "insert_dict, interval_set = add_explicit_insertions(\n",
    "    token_sequences=[[1, 2, 3]],\n",
    "    positions=[1000],\n",
    "    existing_insertions=interval_set\n",
    ")\n",
    "\n",
    "# Try overlapping insertion at position 1002 - this will raise ValueError\n",
    "try:\n",
    "    add_explicit_insertions(\n",
    "        token_sequences=[[4, 5, 6]],\n",
    "        positions=[1002],\n",
    "        existing_insertions=interval_set\n",
    "    )\n",
    "except ValueError as e:\n",
    "    print(f\"ValueError: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random Insertions\n",
    "\n",
    "With random insertions, positions are chosen automatically within a range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 30690.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of inserted tokens: 9\n",
      "Avoided collisions while inserting sequences: 0\n",
      "insert_dict =\n",
      "{\n",
      "    88696: [100, 200],\n",
      "    156013: [1000, 2000, 3000],\n",
      "    173804: [42, 43, 44, 45],\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "insert_dict, _ = add_random_insertions(\n",
    "    token_sequences=token_sequences,\n",
    "    start_idx=0,\n",
    "    end_idx=50 * SEQUENCE_LENGTH,\n",
    "    sequence_length=SEQUENCE_LENGTH,\n",
    "    existing_insertions=IntervalSet(),\n",
    "    rng=np.random.default_rng(seed=42)\n",
    ")\n",
    "\n",
    "print(\"insert_dict =\")\n",
    "print(\"{\")\n",
    "for pos, tokens in sorted(insert_dict.items()):\n",
    "    print(f\"    {pos}: {tokens},\")\n",
    "print(\"}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. With EOS Wrapping\n",
    "\n",
    "Typically you wrap sequences with EOS tokens before insertion to cleanly separate them from surrounding content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum sequence length: 2\n",
      "Maximum sequence length: 4\n",
      "Second maximum sequence length: 3\n",
      "Dropped 0 overly long sequences (longer than 4096 tokens).\n",
      "Dropped 0 empty sequences.\n",
      "Minimum sequence length after wrapping: 4\n",
      "Maximum sequence length after wrapping: 6\n",
      "Second maximum sequence length after wrapping: 5\n",
      "Before wrapping:\n",
      "  [1000, 2000, 3000]\n",
      "  [100, 200]\n",
      "  [42, 43, 44, 45]\n",
      "\n",
      "After wrapping (EOS = 100257):\n",
      "  [100257, 1000, 2000, 3000, 100257]\n",
      "  [100257, 100, 200, 100257]\n",
      "  [100257, 42, 43, 44, 45, 100257]\n"
     ]
    }
   ],
   "source": [
    "# First wrap with EOS tokens\n",
    "wrapped = wrap_sequences_in_eos_tokens(token_sequences, SEQUENCE_LENGTH, EOS_TOKEN_ID)\n",
    "\n",
    "print(\"Before wrapping:\")\n",
    "for seq in token_sequences:\n",
    "    print(f\"  {seq}\")\n",
    "\n",
    "print(\"\\nAfter wrapping (EOS = 100257):\")\n",
    "for seq in wrapped:\n",
    "    print(f\"  {seq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 34007.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of inserted tokens: 15\n",
      "Avoided collisions while inserting sequences: 0\n",
      "Final insert_dict (with EOS tokens):\n",
      "{\n",
      "    88695: [100257, 100, 200, 100257],\n",
      "    156013: [100257, 1000, 2000, 3000, 100257],\n",
      "    173803: [100257, 42, 43, 44, 45, 100257],\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Then insert\n",
    "insert_dict, _ = add_random_insertions(\n",
    "    token_sequences=wrapped,\n",
    "    start_idx=0,\n",
    "    end_idx=50 * SEQUENCE_LENGTH,\n",
    "    sequence_length=SEQUENCE_LENGTH,\n",
    "    existing_insertions=IntervalSet(),\n",
    "    rng=np.random.default_rng(seed=42)\n",
    ")\n",
    "\n",
    "print(\"Final insert_dict (with EOS tokens):\")\n",
    "print(\"{\")\n",
    "for pos, tokens in sorted(insert_dict.items()):\n",
    "    print(f\"    {pos}: {tokens},\")\n",
    "print(\"}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Combining Explicit and Random\n",
    "\n",
    "Process explicit insertions first, then random. The random insertions automatically avoid the explicit positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 28149.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of inserted tokens: 4\n",
      "Avoided collisions while inserting sequences: 0\n",
      "Combined insert_dict:\n",
      "{\n",
      "    8192: [1, 2, 3],  # explicit\n",
      "    19064: [30, 40],  # random\n",
      "    29037: [10, 20],  # random\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "interval_set = IntervalSet()\n",
    "\n",
    "# Phase 1: Explicit\n",
    "explicit_dict, interval_set = add_explicit_insertions(\n",
    "    token_sequences=[[1, 2, 3]],\n",
    "    positions=[8192],  # Exactly at position 8192\n",
    "    existing_insertions=interval_set\n",
    ")\n",
    "\n",
    "# Phase 2: Random (uses the same interval_set, so won't overlap)\n",
    "random_dict, interval_set = add_random_insertions(\n",
    "    token_sequences=[[10, 20], [30, 40]],\n",
    "    start_idx=0,\n",
    "    end_idx=10 * SEQUENCE_LENGTH,\n",
    "    sequence_length=SEQUENCE_LENGTH,\n",
    "    existing_insertions=interval_set,\n",
    "    rng=np.random.default_rng(seed=42)\n",
    ")\n",
    "\n",
    "# Combine\n",
    "insert_dict = {**explicit_dict, **random_dict}\n",
    "\n",
    "print(\"Combined insert_dict:\")\n",
    "print(\"{\")\n",
    "for pos, tokens in sorted(insert_dict.items()):\n",
    "    source = \"explicit\" if pos in explicit_dict else \"random\"\n",
    "    print(f\"    {pos}: {tokens},  # {source}\")\n",
    "print(\"}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary of Step 1\n\nThe `insert_dict` is a simple mapping from global token positions to token sequences:\n\n```\n{position: tokens, position: tokens, ...}\n```\n\nThis is the **framework-agnostic** representation. Next, we convert it to an indexed format for efficient storage and framework-specific use."
  },
  {
   "cell_type": "markdown",
   "source": "# Step 2: Converting to Index-Based Format",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "While the `insert_dict` structure is great for specifying insertions, it is not straightforward to integrate into dataloaders in an efficient way. Dataloaders process data in chunks (sequences, batches), and looking up insertions by global position would require scanning all positions for every chunk.\n\nInstead, we convert to an **index-based format** that groups insertions by chunk:\n\n```python\nindex_map = {\n    index: [(local_position, [tokens]), (local_position, [tokens]), ...],\n    index: [(local_position, [tokens]), ...],\n    ...\n}\n```\n\nWhere:\n- `index` is the chunk number (e.g., sequence index, batch index)\n- `local_position` is the position within that chunk\n- `[tokens]` is the token sequence to insert\n\nThis allows O(1) lookup: when processing chunk N, just check if `index_map[N]` exists. The `InsertionMapWriter` stores this format in HDF5 files for efficient random access during training.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from pretrain_experiments.token_insertion import convert_insert_dict_to_index_map\nfrom pretrain_experiments.insertion_map import InsertionMapWriter, InsertionMapReader\nimport tempfile\nimport os",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Converting with `convert_insert_dict_to_index_map`\n\nThe `num_index_tokens` parameter controls how positions are grouped:\n- For **per-sequence** indexing: `num_index_tokens = sequence_length` (e.g., 4096)\n- For **per-batch** indexing: `num_index_tokens = batch_size * sequence_length`",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create an insert_dict with positions across multiple sequences\ninsert_dict = {\n    1000: [1, 2, 3],         # Sequence 0, position 1000\n    5000: [4, 5, 6, 7],      # Sequence 1, position 904 (5000 - 4096)\n    10000: [8, 9],           # Sequence 2, position 1808 (10000 - 2*4096)\n}\n\n# Convert to index-based format (per-sequence)\nindex_map = convert_insert_dict_to_index_map(\n    insert_dict,\n    num_index_tokens=SEQUENCE_LENGTH  # 4096 tokens per sequence\n)\n\nprint(\"Original insert_dict (global positions):\")\nprint(\"{\")\nfor pos, tokens in sorted(insert_dict.items()):\n    print(f\"    {pos}: {tokens},\")\nprint(\"}\")\n\nprint(\"\\nConverted index_map (sequence-indexed):\")\nprint(\"{\")\nfor idx, insertions in sorted(index_map.items()):\n    print(f\"    {idx}: {insertions},\")\nprint(\"}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Handling Boundary Crossings\n\nWhen an insertion crosses an index boundary, it can be automatically split:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Insertion at position 4094 with 5 tokens crosses into the next sequence\ninsert_dict_crossing = {\n    4094: [1, 2, 3, 4, 5],  # Starts in seq 0, ends in seq 1\n}\n\nindex_map_split = convert_insert_dict_to_index_map(\n    insert_dict_crossing,\n    num_index_tokens=SEQUENCE_LENGTH,\n    split_across_boundaries=True  # Default behavior\n)\n\nprint(\"Original: position 4094, tokens [1, 2, 3, 4, 5]\")\nprint(f\"Sequence boundary at position 4096\\n\")\nprint(\"After splitting:\")\nfor idx, insertions in sorted(index_map_split.items()):\n    for pos, tokens in insertions:\n        print(f\"  Sequence {idx}, position {pos}: {tokens}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Storing in HDF5 with InsertionMapWriter\n\nFor large-scale training, we store the index_map in HDF5 format. This provides:\n- Efficient random access (O(1) lookup by index)\n- Compression for token sequences\n- Incremental writes without loading entire file",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create a sample index_map\ninsert_dict = {\n    1000: [1, 2, 3],\n    5000: [4, 5, 6, 7],\n    10000: [8, 9],\n    12500: [10, 11, 12],\n}\n\nindex_map = convert_insert_dict_to_index_map(insert_dict, num_index_tokens=SEQUENCE_LENGTH)\n\n# Write to HDF5\nwith tempfile.TemporaryDirectory() as tmpdir:\n    hdf5_path = os.path.join(tmpdir, \"insertions.h5\")\n    \n    # Write the index_map\n    writer = InsertionMapWriter(hdf5_path)\n    writer.write_dict(index_map)\n    \n    # Read it back\n    with InsertionMapReader(hdf5_path) as reader:\n        print(f\"Stored {len(reader)} indices in HDF5\\n\")\n        \n        # Check which indices have insertions\n        print(\"Indices with insertions:\", reader.get_all_indices())\n        \n        # Load insertions for a specific index\n        print(\"\\nInsertions for sequence 1:\")\n        insertions = reader.load(1)\n        if insertions:\n            for pos, tokens in insertions:\n                print(f\"  Position {pos}: {tokens}\")\n        \n        # Fast membership check (no file I/O after initial load)\n        print(f\"\\nSequence 0 has insertions: {0 in reader}\")\n        print(f\"Sequence 5 has insertions: {5 in reader}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Summary\n\nThe complete pipeline for token insertions:\n\n1. **Create insert_dict** using `add_explicit_insertions()` or `add_random_insertions()`\n   - Maps global token positions to token sequences\n   - Use `IntervalSet` to prevent overlapping insertions\n\n2. **Convert to index_map** using `convert_insert_dict_to_index_map()`\n   - Groups insertions by sequence/batch index for efficient lookup\n   - Handles boundary crossings with automatic splitting\n\n3. **Store in HDF5** using `InsertionMapWriter`\n   - Efficient random access during training\n   - Use `InsertionMapReader` in your dataloader to lookup insertions by index",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "olmo-core",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}